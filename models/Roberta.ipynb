{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb51a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "417f468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"itog_data.csv\")\n",
    "data_eval = pd.read_csv(\"train_data.csv\")\n",
    "\n",
    "# data['combined_questions'] = data['question_1'] + ' ' + data['question_2'] + ' ' + data['question_3'] + ' ' + data['question_4'] + ' ' + data['question_5']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "93651baf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c672782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('russian')\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"[^а-яА-Я?.!,¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "\n",
    "    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs \n",
    "    #text = re.sub(r\"http\", \"\",text)\n",
    "    \n",
    "    html=re.compile(r'<.*?>') \n",
    "    \n",
    "    text = html.sub(r'',text) #Removing html tags\n",
    "    \n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
    "    for p in punctuations:\n",
    "        text = text.replace(p,'') #Removing punctuations\n",
    "        \n",
    "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
    "    \n",
    "    text = \" \".join(text) #removing stopwords\n",
    "    \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ea80c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['question_1'] = data['question_1'].apply(lambda x: clean_text(x))\n",
    "data['question_2'] = data['question_2'].apply(lambda x: clean_text(x))\n",
    "data['question_3'] = data['question_3'].apply(lambda x: clean_text(x))\n",
    "data['question_4'] = data['question_4'].apply(lambda x: clean_text(x))\n",
    "data['question_5'] = data['question_5'].apply(lambda x: clean_text(x))\n",
    "\n",
    "\n",
    "data_eval['question_1'] = data_eval['question_1'].apply(lambda x: clean_text(x))\n",
    "data_eval['question_2'] = data_eval['question_2'].apply(lambda x: clean_text(x))\n",
    "data_eval['question_3'] = data_eval['question_3'].apply(lambda x: clean_text(x))\n",
    "data_eval['question_4'] = data_eval['question_4'].apply(lambda x: clean_text(x))\n",
    "data_eval['question_5'] = data_eval['question_5'].apply(lambda x: clean_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "88f32520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>question_1</th>\n",
       "      <th>question_2</th>\n",
       "      <th>question_3</th>\n",
       "      <th>question_4</th>\n",
       "      <th>question_5</th>\n",
       "      <th>is_relevant</th>\n",
       "      <th>object</th>\n",
       "      <th>is_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-16 09:30:00</td>\n",
       "      <td>основы программирования</td>\n",
       "      <td>преподаватель смог заинтересовать</td>\n",
       "      <td>неинтересно</td>\n",
       "      <td>стоит пытаться улучшать, просто отменить</td>\n",
       "      <td>жуть</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2024-04-17 14:45:00</td>\n",
       "      <td>основы программирования</td>\n",
       "      <td>потерял часа вебинаре</td>\n",
       "      <td>понял, шла речь</td>\n",
       "      <td>проводили</td>\n",
       "      <td>безнадежность</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2024-04-18 11:20:00</td>\n",
       "      <td>основы программирования</td>\n",
       "      <td>потеря времени</td>\n",
       "      <td>пропустил половину абсолютной нудятины</td>\n",
       "      <td>таких вебинаров следует избегать</td>\n",
       "      <td>полное разочарование</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2024-04-19 16:55:00</td>\n",
       "      <td>основы программирования</td>\n",
       "      <td>просто ужас</td>\n",
       "      <td>запомнил</td>\n",
       "      <td>полное отсутствие смысла</td>\n",
       "      <td>просто кошмар</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2024-04-20 13:10:00</td>\n",
       "      <td>основы программирования</td>\n",
       "      <td>никакой пользы извлек</td>\n",
       "      <td>потерял всякий интерес</td>\n",
       "      <td>поспал это время</td>\n",
       "      <td>ужасное впечатление</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1310</th>\n",
       "      <td>999</td>\n",
       "      <td>2024-04-07 07:54:02</td>\n",
       "      <td>основы программирования</td>\n",
       "      <td>заинтересовало, вебинаре обсуждался инструмент...</td>\n",
       "      <td>да, некоторые проблемы объяснением некоторых а...</td>\n",
       "      <td>кажется, стоит времени уделить рассмотрению те...</td>\n",
       "      <td>хотелось узнать библиотеках</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1311</th>\n",
       "      <td>1000</td>\n",
       "      <td>2024-03-29 05:55:08</td>\n",
       "      <td>продвинутые техники программирования</td>\n",
       "      <td>вебинар проектировании оказался очень полезным</td>\n",
       "      <td>согласен, примеры рефакторинга могли сложными ...</td>\n",
       "      <td>рабочие листы помогут усвоить материал</td>\n",
       "      <td>действительно узнал вебинара принципах проекти...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>1001</td>\n",
       "      <td>2024-04-24 20:25:25</td>\n",
       "      <td>управление персоналом</td>\n",
       "      <td>превосходно</td>\n",
       "      <td>расписание оказалось неудобным пересечения дру...</td>\n",
       "      <td>никаких нареканий</td>\n",
       "      <td>чат студентами оказался полезным дополнением</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>1002</td>\n",
       "      <td>2024-04-22 19:02:00</td>\n",
       "      <td>новейшие тенденции</td>\n",
       "      <td>вряд заинтересовало вебинаре новейшие тенденци...</td>\n",
       "      <td>возможно, вебинаре новейшие тенденции моменты,...</td>\n",
       "      <td>быть, вебинаре новейшие тенденции моменты, кот...</td>\n",
       "      <td>интересуют другие темы, связанные преподавател...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314</th>\n",
       "      <td>1003</td>\n",
       "      <td>2024-04-22 19:02:00</td>\n",
       "      <td>новейшие тенденции</td>\n",
       "      <td>вебинаре новейшие тенденции паттерны проектиро...</td>\n",
       "      <td>трудно понять вебинаре новейшие тенденции , эт...</td>\n",
       "      <td>вебинаре новейшие тенденции наверное, улучшить...</td>\n",
       "      <td>будущих вебинарах хотел изучить интересные тем...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1315 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0            timestamp                            question_1  \\\n",
       "0              0  2024-04-16 09:30:00               основы программирования   \n",
       "1              1  2024-04-17 14:45:00               основы программирования   \n",
       "2              2  2024-04-18 11:20:00               основы программирования   \n",
       "3              3  2024-04-19 16:55:00               основы программирования   \n",
       "4              4  2024-04-20 13:10:00               основы программирования   \n",
       "...          ...                  ...                                   ...   \n",
       "1310         999  2024-04-07 07:54:02               основы программирования   \n",
       "1311        1000  2024-03-29 05:55:08  продвинутые техники программирования   \n",
       "1312        1001  2024-04-24 20:25:25                 управление персоналом   \n",
       "1313        1002  2024-04-22 19:02:00                    новейшие тенденции   \n",
       "1314        1003  2024-04-22 19:02:00                    новейшие тенденции   \n",
       "\n",
       "                                             question_2  \\\n",
       "0                     преподаватель смог заинтересовать   \n",
       "1                                 потерял часа вебинаре   \n",
       "2                                        потеря времени   \n",
       "3                                           просто ужас   \n",
       "4                                 никакой пользы извлек   \n",
       "...                                                 ...   \n",
       "1310  заинтересовало, вебинаре обсуждался инструмент...   \n",
       "1311     вебинар проектировании оказался очень полезным   \n",
       "1312                                        превосходно   \n",
       "1313  вряд заинтересовало вебинаре новейшие тенденци...   \n",
       "1314  вебинаре новейшие тенденции паттерны проектиро...   \n",
       "\n",
       "                                             question_3  \\\n",
       "0                                           неинтересно   \n",
       "1                                       понял, шла речь   \n",
       "2                пропустил половину абсолютной нудятины   \n",
       "3                                              запомнил   \n",
       "4                                потерял всякий интерес   \n",
       "...                                                 ...   \n",
       "1310  да, некоторые проблемы объяснением некоторых а...   \n",
       "1311  согласен, примеры рефакторинга могли сложными ...   \n",
       "1312  расписание оказалось неудобным пересечения дру...   \n",
       "1313  возможно, вебинаре новейшие тенденции моменты,...   \n",
       "1314  трудно понять вебинаре новейшие тенденции , эт...   \n",
       "\n",
       "                                             question_4  \\\n",
       "0              стоит пытаться улучшать, просто отменить   \n",
       "1                                             проводили   \n",
       "2                      таких вебинаров следует избегать   \n",
       "3                              полное отсутствие смысла   \n",
       "4                                      поспал это время   \n",
       "...                                                 ...   \n",
       "1310  кажется, стоит времени уделить рассмотрению те...   \n",
       "1311             рабочие листы помогут усвоить материал   \n",
       "1312                                  никаких нареканий   \n",
       "1313  быть, вебинаре новейшие тенденции моменты, кот...   \n",
       "1314  вебинаре новейшие тенденции наверное, улучшить...   \n",
       "\n",
       "                                             question_5  is_relevant  object  \\\n",
       "0                                                  жуть            0       0   \n",
       "1                                         безнадежность            0       0   \n",
       "2                                  полное разочарование            0       0   \n",
       "3                                         просто кошмар            0       0   \n",
       "4                                   ужасное впечатление            0       0   \n",
       "...                                                 ...          ...     ...   \n",
       "1310                        хотелось узнать библиотеках            1       1   \n",
       "1311  действительно узнал вебинара принципах проекти...            1       1   \n",
       "1312       чат студентами оказался полезным дополнением            1       1   \n",
       "1313  интересуют другие темы, связанные преподавател...            0       2   \n",
       "1314  будущих вебинарах хотел изучить интересные тем...            0       2   \n",
       "\n",
       "      is_positive  \n",
       "0             0.0  \n",
       "1             0.0  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  \n",
       "...           ...  \n",
       "1310          0.0  \n",
       "1311          1.0  \n",
       "1312          0.0  \n",
       "1313          0.0  \n",
       "1314          0.0  \n",
       "\n",
       "[1315 rows x 10 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8dc2a0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 24\n",
    "VALID_BATCH_SIZE = 48\n",
    "# EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b28b077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('ai-forever/ruRoberta-large', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cbdc04f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_is_relevant = data.copy()\n",
    "data_object = data.copy()\n",
    "data_is_positive = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60e8742",
   "metadata": {},
   "source": [
    "# is_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f8928b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe['question_1'] + \" \" + dataframe['question_2'] + \" \" + dataframe['question_3'] + \" \" + dataframe['question_4'] + \" \" + dataframe['question_5']\n",
    "        self.targets = self.data['is_relevant']\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d92473c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (1315, 10)\n",
      "TRAIN Dataset: (1052, 10)\n",
      "TEST Dataset: (263, 10)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_data=data_is_relevant.sample(frac=train_size,random_state=42)\n",
    "\n",
    "test_data=data_is_relevant.drop(train_data.index)\n",
    "test_data_index = test_data.index.tolist()\n",
    "test_data=test_data.reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data_is_relevant.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = SentimentData(data, tokenizer, MAX_LEN)\n",
    "testing_set = SentimentData(data_eval, tokenizer, MAX_LEN)\n",
    "all_data_set = SentimentData(data_is_relevant, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c40f28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "all_df_params = {'batch_size': 24,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "all_data_loader = DataLoader(all_data_set, **all_df_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "09680384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "class is_relevant(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(is_relevant, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained('ai-forever/ruRoberta-large')\n",
    "        self.pre_classifier = torch.nn.Linear(1024, 1024)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(1024, 2) # 4 classes for category\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "    \n",
    "    def get_embed(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        return pooler\n",
    "    \n",
    "model_is_relevant = is_relevant()\n",
    "model_is_relevant.to(device)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "76feb4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "LEARNING_RATE = 1e-05\n",
    "optimizer = torch.optim.Adam(params =  model_is_relevant.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5ef2809e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.652361273765564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:39,  1.38it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3,Training Loss Epoch: 0.5007578985257582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.5553439855575562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:40,  1.34it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3,Training Loss Epoch: 0.6414019725539467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 1.7998837232589722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:41,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3,Training Loss Epoch: 0.6888151390986009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(0, epochs):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model_is_relevant.train()\n",
    "    for _,data_train in tqdm(enumerate(all_data_loader, 0)):\n",
    "        ids = data_train['ids'].to(device, dtype = torch.long)\n",
    "        mask = data_train['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data_train['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data_train['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model_is_relevant(ids, mask, token_type_ids)\n",
    "        num_classes = 2  # Два класса\n",
    "        targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=num_classes)\n",
    "        targets_one_hot = targets_one_hot.float()\n",
    "        outputs = outputs.float()\n",
    "        loss = loss_function(outputs, targets_one_hot)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        y_preds = label_binarize(big_idx.cpu().numpy(), classes=[0,1])\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps     \n",
    "    print(f\"Epoch {epoch+1}/{epochs},Training Loss Epoch: {epoch_loss}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6ee6e9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:01,  2.10it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs_all = []\n",
    "targets_all = []\n",
    "with torch.no_grad():\n",
    "    for _,data_res in tqdm(enumerate(testing_loader, 0)):\n",
    "        ids = data_res['ids'].to(device, dtype = torch.long)\n",
    "        mask = data_res['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data_res['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data_res['targets'].to(device, dtype = torch.long)\n",
    "        embed = model_is_relevant.get_embed(ids, mask, token_type_ids)\n",
    "        outputs_all += embed.cpu().numpy().tolist()\n",
    "        targets_all += targets.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0951da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed_is_relevant = pd.DataFrame(outputs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "525f2438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1014</th>\n",
       "      <th>1015</th>\n",
       "      <th>1016</th>\n",
       "      <th>1017</th>\n",
       "      <th>1018</th>\n",
       "      <th>1019</th>\n",
       "      <th>1020</th>\n",
       "      <th>1021</th>\n",
       "      <th>1022</th>\n",
       "      <th>1023</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.081664</td>\n",
       "      <td>0.279511</td>\n",
       "      <td>-1.306082</td>\n",
       "      <td>-0.048281</td>\n",
       "      <td>0.999499</td>\n",
       "      <td>-1.936790</td>\n",
       "      <td>-0.291874</td>\n",
       "      <td>0.627558</td>\n",
       "      <td>-0.192689</td>\n",
       "      <td>-0.232225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445116</td>\n",
       "      <td>-0.063830</td>\n",
       "      <td>0.138965</td>\n",
       "      <td>-0.343812</td>\n",
       "      <td>-0.585849</td>\n",
       "      <td>-0.177153</td>\n",
       "      <td>-0.014231</td>\n",
       "      <td>0.115830</td>\n",
       "      <td>0.626986</td>\n",
       "      <td>0.549741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.460130</td>\n",
       "      <td>0.599713</td>\n",
       "      <td>-1.429861</td>\n",
       "      <td>-0.213349</td>\n",
       "      <td>0.330546</td>\n",
       "      <td>-1.774535</td>\n",
       "      <td>-0.550755</td>\n",
       "      <td>1.276812</td>\n",
       "      <td>0.100365</td>\n",
       "      <td>-0.643270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.806015</td>\n",
       "      <td>-0.308411</td>\n",
       "      <td>0.400529</td>\n",
       "      <td>0.057245</td>\n",
       "      <td>-1.349308</td>\n",
       "      <td>-0.300856</td>\n",
       "      <td>-0.226646</td>\n",
       "      <td>0.112459</td>\n",
       "      <td>0.889113</td>\n",
       "      <td>0.837405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.539640</td>\n",
       "      <td>-0.426453</td>\n",
       "      <td>-1.198697</td>\n",
       "      <td>-0.152789</td>\n",
       "      <td>0.434318</td>\n",
       "      <td>-2.296316</td>\n",
       "      <td>-0.424175</td>\n",
       "      <td>1.190953</td>\n",
       "      <td>-0.005965</td>\n",
       "      <td>-0.191491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.402574</td>\n",
       "      <td>-0.946041</td>\n",
       "      <td>0.601490</td>\n",
       "      <td>0.302094</td>\n",
       "      <td>-0.450755</td>\n",
       "      <td>-0.036583</td>\n",
       "      <td>0.434536</td>\n",
       "      <td>0.961563</td>\n",
       "      <td>1.493554</td>\n",
       "      <td>0.235780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.663000</td>\n",
       "      <td>0.160556</td>\n",
       "      <td>-0.733101</td>\n",
       "      <td>-0.633255</td>\n",
       "      <td>0.396868</td>\n",
       "      <td>-1.729588</td>\n",
       "      <td>0.071108</td>\n",
       "      <td>1.353120</td>\n",
       "      <td>0.927181</td>\n",
       "      <td>-0.845682</td>\n",
       "      <td>...</td>\n",
       "      <td>1.347812</td>\n",
       "      <td>0.147930</td>\n",
       "      <td>-0.462270</td>\n",
       "      <td>-0.871620</td>\n",
       "      <td>-0.836188</td>\n",
       "      <td>-0.135969</td>\n",
       "      <td>-0.004095</td>\n",
       "      <td>-0.184775</td>\n",
       "      <td>0.903543</td>\n",
       "      <td>0.374902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.726380</td>\n",
       "      <td>0.488604</td>\n",
       "      <td>-1.728621</td>\n",
       "      <td>-0.304118</td>\n",
       "      <td>0.171412</td>\n",
       "      <td>-1.156725</td>\n",
       "      <td>-0.647366</td>\n",
       "      <td>1.139533</td>\n",
       "      <td>0.579269</td>\n",
       "      <td>-0.372429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105383</td>\n",
       "      <td>-0.158838</td>\n",
       "      <td>0.184433</td>\n",
       "      <td>-1.111090</td>\n",
       "      <td>-0.336094</td>\n",
       "      <td>-0.509820</td>\n",
       "      <td>-0.395262</td>\n",
       "      <td>-0.159298</td>\n",
       "      <td>1.061503</td>\n",
       "      <td>-0.367074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.676045</td>\n",
       "      <td>-0.336093</td>\n",
       "      <td>-1.516252</td>\n",
       "      <td>-0.549135</td>\n",
       "      <td>0.268364</td>\n",
       "      <td>-1.927011</td>\n",
       "      <td>-0.167268</td>\n",
       "      <td>1.328393</td>\n",
       "      <td>0.217344</td>\n",
       "      <td>-0.776154</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629019</td>\n",
       "      <td>-0.515349</td>\n",
       "      <td>0.694408</td>\n",
       "      <td>-0.438487</td>\n",
       "      <td>-0.874299</td>\n",
       "      <td>-0.014681</td>\n",
       "      <td>-0.161659</td>\n",
       "      <td>0.378393</td>\n",
       "      <td>0.777127</td>\n",
       "      <td>0.517771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.027436</td>\n",
       "      <td>0.422550</td>\n",
       "      <td>-1.127410</td>\n",
       "      <td>-0.094676</td>\n",
       "      <td>0.630936</td>\n",
       "      <td>-2.311038</td>\n",
       "      <td>-0.346903</td>\n",
       "      <td>1.028845</td>\n",
       "      <td>0.125721</td>\n",
       "      <td>0.189822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.556769</td>\n",
       "      <td>-0.058344</td>\n",
       "      <td>0.261299</td>\n",
       "      <td>-0.615340</td>\n",
       "      <td>-0.511161</td>\n",
       "      <td>0.098427</td>\n",
       "      <td>-0.211003</td>\n",
       "      <td>-0.131077</td>\n",
       "      <td>1.001691</td>\n",
       "      <td>0.318375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.682844</td>\n",
       "      <td>1.225882</td>\n",
       "      <td>-1.169281</td>\n",
       "      <td>-0.170147</td>\n",
       "      <td>0.732660</td>\n",
       "      <td>-2.141634</td>\n",
       "      <td>0.395839</td>\n",
       "      <td>1.414442</td>\n",
       "      <td>0.330094</td>\n",
       "      <td>-0.839418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.787654</td>\n",
       "      <td>-0.464076</td>\n",
       "      <td>-0.359345</td>\n",
       "      <td>-1.019804</td>\n",
       "      <td>-1.196739</td>\n",
       "      <td>-0.299736</td>\n",
       "      <td>-0.329388</td>\n",
       "      <td>-0.029063</td>\n",
       "      <td>1.027460</td>\n",
       "      <td>0.066824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.273941</td>\n",
       "      <td>0.356838</td>\n",
       "      <td>-0.971498</td>\n",
       "      <td>-0.421066</td>\n",
       "      <td>-0.259383</td>\n",
       "      <td>-1.927647</td>\n",
       "      <td>-0.098157</td>\n",
       "      <td>0.972594</td>\n",
       "      <td>0.378320</td>\n",
       "      <td>-0.123138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499048</td>\n",
       "      <td>-0.133155</td>\n",
       "      <td>0.349798</td>\n",
       "      <td>-0.549505</td>\n",
       "      <td>-0.928472</td>\n",
       "      <td>-0.069889</td>\n",
       "      <td>-0.051815</td>\n",
       "      <td>0.646629</td>\n",
       "      <td>0.718018</td>\n",
       "      <td>0.014646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-0.168152</td>\n",
       "      <td>0.694642</td>\n",
       "      <td>-1.395494</td>\n",
       "      <td>-0.326439</td>\n",
       "      <td>1.108144</td>\n",
       "      <td>-2.026098</td>\n",
       "      <td>-0.086509</td>\n",
       "      <td>1.618883</td>\n",
       "      <td>0.020245</td>\n",
       "      <td>-0.487536</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642110</td>\n",
       "      <td>-0.416790</td>\n",
       "      <td>0.383876</td>\n",
       "      <td>0.089238</td>\n",
       "      <td>-1.019342</td>\n",
       "      <td>0.072482</td>\n",
       "      <td>0.170393</td>\n",
       "      <td>0.457068</td>\n",
       "      <td>0.731128</td>\n",
       "      <td>0.471632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 1024 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.081664  0.279511 -1.306082 -0.048281  0.999499 -1.936790 -0.291874   \n",
       "1    1.460130  0.599713 -1.429861 -0.213349  0.330546 -1.774535 -0.550755   \n",
       "2    0.539640 -0.426453 -1.198697 -0.152789  0.434318 -2.296316 -0.424175   \n",
       "3    0.663000  0.160556 -0.733101 -0.633255  0.396868 -1.729588  0.071108   \n",
       "4    0.726380  0.488604 -1.728621 -0.304118  0.171412 -1.156725 -0.647366   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "98   0.676045 -0.336093 -1.516252 -0.549135  0.268364 -1.927011 -0.167268   \n",
       "99   0.027436  0.422550 -1.127410 -0.094676  0.630936 -2.311038 -0.346903   \n",
       "100  0.682844  1.225882 -1.169281 -0.170147  0.732660 -2.141634  0.395839   \n",
       "101  1.273941  0.356838 -0.971498 -0.421066 -0.259383 -1.927647 -0.098157   \n",
       "102 -0.168152  0.694642 -1.395494 -0.326439  1.108144 -2.026098 -0.086509   \n",
       "\n",
       "         7         8         9     ...      1014      1015      1016  \\\n",
       "0    0.627558 -0.192689 -0.232225  ...  0.445116 -0.063830  0.138965   \n",
       "1    1.276812  0.100365 -0.643270  ...  0.806015 -0.308411  0.400529   \n",
       "2    1.190953 -0.005965 -0.191491  ...  0.402574 -0.946041  0.601490   \n",
       "3    1.353120  0.927181 -0.845682  ...  1.347812  0.147930 -0.462270   \n",
       "4    1.139533  0.579269 -0.372429  ...  0.105383 -0.158838  0.184433   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "98   1.328393  0.217344 -0.776154  ...  0.629019 -0.515349  0.694408   \n",
       "99   1.028845  0.125721  0.189822  ... -0.556769 -0.058344  0.261299   \n",
       "100  1.414442  0.330094 -0.839418  ...  0.787654 -0.464076 -0.359345   \n",
       "101  0.972594  0.378320 -0.123138  ...  0.499048 -0.133155  0.349798   \n",
       "102  1.618883  0.020245 -0.487536  ...  0.642110 -0.416790  0.383876   \n",
       "\n",
       "         1017      1018      1019      1020      1021      1022      1023  \n",
       "0   -0.343812 -0.585849 -0.177153 -0.014231  0.115830  0.626986  0.549741  \n",
       "1    0.057245 -1.349308 -0.300856 -0.226646  0.112459  0.889113  0.837405  \n",
       "2    0.302094 -0.450755 -0.036583  0.434536  0.961563  1.493554  0.235780  \n",
       "3   -0.871620 -0.836188 -0.135969 -0.004095 -0.184775  0.903543  0.374902  \n",
       "4   -1.111090 -0.336094 -0.509820 -0.395262 -0.159298  1.061503 -0.367074  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "98  -0.438487 -0.874299 -0.014681 -0.161659  0.378393  0.777127  0.517771  \n",
       "99  -0.615340 -0.511161  0.098427 -0.211003 -0.131077  1.001691  0.318375  \n",
       "100 -1.019804 -1.196739 -0.299736 -0.329388 -0.029063  1.027460  0.066824  \n",
       "101 -0.549505 -0.928472 -0.069889 -0.051815  0.646629  0.718018  0.014646  \n",
       "102  0.089238 -1.019342  0.072482  0.170393  0.457068  0.731128  0.471632  \n",
       "\n",
       "[103 rows x 1024 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embed_is_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "982d6b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed_is_relevant.to_csv('df_embed_is_relevant.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0989d8",
   "metadata": {},
   "source": [
    "# is_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ce57de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class is_positiveData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe['question_1'] + \" \" + dataframe['question_2'] + \" \" + dataframe['question_3'] + \" \" + dataframe['question_4'] + \" \" + dataframe['question_5']\n",
    "        self.targets = self.data['is_positive']\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0455682f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (1315, 10)\n",
      "TRAIN Dataset: (1052, 10)\n",
      "TEST Dataset: (263, 10)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_data=data_is_positive.sample(frac=train_size,random_state=42)\n",
    "\n",
    "test_data=data_is_positive.drop(train_data.index)\n",
    "test_data_index = test_data.index.tolist()\n",
    "test_data=test_data.reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data_is_positive.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = is_positiveData(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = is_positiveData(test_data, tokenizer, MAX_LEN)\n",
    "all_data_set = is_positiveData(data_is_positive, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9b7863e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "all_df_params = {'batch_size': 24,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "all_data_loader = DataLoader(all_data_set, **all_df_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12d0c65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "class is_positive(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(is_positive, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained('ai-forever/ruRoberta-large')\n",
    "        self.pre_classifier = torch.nn.Linear(1024, 1024)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(1024, 2) # 4 classes for category\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "    \n",
    "    def get_embed(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        return pooler\n",
    "    \n",
    "model_is_positive = is_positive()\n",
    "model_is_positive.to(device)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40c8ed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "LEARNING_RATE = 1e-05\n",
    "optimizer = torch.optim.Adam(params =  model_is_relevant.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e607c5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6803669929504395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:39,  1.39it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3,Training Loss Epoch: 0.7161724404855208\n",
      "Training Loss per 5000 steps: 0.6916036605834961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:39,  1.38it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3,Training Loss Epoch: 0.7179316834969954\n",
      "Training Loss per 5000 steps: 0.6880072951316833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:40,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3,Training Loss Epoch: 0.7144191611896862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(0, epochs):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model_is_positive.train()\n",
    "    for _,data_train in tqdm(enumerate(all_data_loader, 0)):\n",
    "        ids = data_train['ids'].to(device, dtype = torch.long)\n",
    "        mask = data_train['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data_train['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data_train['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model_is_positive(ids, mask, token_type_ids)\n",
    "        num_classes = 2  # Два класса\n",
    "        targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=num_classes)\n",
    "        targets_one_hot = targets_one_hot.float()\n",
    "        outputs = outputs.float()\n",
    "        loss = loss_function(outputs, targets_one_hot)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        y_preds = label_binarize(big_idx.cpu().numpy(), classes=[0,1])\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps     \n",
    "    print(f\"Epoch {epoch+1}/{epochs},Training Loss Epoch: {epoch_loss}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "24a98179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:15,  3.50it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs_all = []\n",
    "targets_all = []\n",
    "with torch.no_grad():\n",
    "    for _,data_res in tqdm(enumerate(all_data_loader, 0)):\n",
    "        ids = data_res['ids'].to(device, dtype = torch.long)\n",
    "        mask = data_res['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data_res['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data_res['targets'].to(device, dtype = torch.long)\n",
    "        embed = model_is_positive.get_embed(ids, mask, token_type_ids)\n",
    "        outputs_all += embed.cpu().numpy().tolist()\n",
    "        targets_all += targets.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "141b2faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed_is_positive = pd.DataFrame(outputs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3335dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed_is_positive.to_csv('df_embed_is_positive.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa37b1",
   "metadata": {},
   "source": [
    "# object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7cf09714",
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectData(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe['question_1'] + \" \" + dataframe['question_2'] + \" \" + dataframe['question_3'] + \" \" + dataframe['question_4'] + \" \" + dataframe['question_5']\n",
    "        self.targets = self.data['object']\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "92c0fc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (1315, 10)\n",
      "TRAIN Dataset: (1052, 10)\n",
      "TEST Dataset: (263, 10)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "train_data=data_object.sample(frac=train_size,random_state=42)\n",
    "\n",
    "test_data=data_object.drop(train_data.index)\n",
    "test_data_index = test_data.index.tolist()\n",
    "test_data=test_data.reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(data_object.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = objectData(train_data, tokenizer, MAX_LEN)\n",
    "testing_set = objectData(test_data, tokenizer, MAX_LEN)\n",
    "all_data_set = objectData(data_object, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7140342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "all_df_params = {'batch_size': 24,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "all_data_loader = DataLoader(all_data_set, **all_df_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa41022d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "class object_m(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(object_m, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained('ai-forever/ruRoberta-large')\n",
    "        self.pre_classifier = torch.nn.Linear(1024, 1024)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(1024, 3) # 4 classes for category\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "    \n",
    "    def get_embed(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        return pooler\n",
    "    \n",
    "model_object = object_m()\n",
    "model_object.to(device)\n",
    "print('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ba0b9a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.BCEWithLogitsLoss()\n",
    "LEARNING_RATE = 1e-05\n",
    "optimizer = torch.optim.Adam(params =  model_is_relevant.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36e9db43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss per 5000 steps: 0.6499131917953491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:39,  1.39it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3,Training Loss Epoch: 0.6971661307594993\n",
      "Training Loss per 5000 steps: 0.6842942833900452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:39,  1.38it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3,Training Loss Epoch: 0.696126872842962\n",
      "Training Loss per 5000 steps: 0.6698768138885498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:39,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3,Training Loss Epoch: 0.6985671975395896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for epoch in range(0, epochs):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model_object.train()\n",
    "    for _,data_train in tqdm(enumerate(all_data_loader, 0)):\n",
    "        ids = data_train['ids'].to(device, dtype = torch.long)\n",
    "        mask = data_train['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data_train['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data_train['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model_object(ids, mask, token_type_ids)\n",
    "        num_classes = 3  # Два класса\n",
    "        targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=num_classes)\n",
    "        targets_one_hot = targets_one_hot.float()\n",
    "        outputs = outputs.float()\n",
    "        loss = loss_function(outputs, targets_one_hot)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        y_preds = label_binarize(big_idx.cpu().numpy(), classes=[0,1,2])\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%5000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training Loss per 5000 steps: {loss_step}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # # When using GPU\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps     \n",
    "    print(f\"Epoch {epoch+1}/{epochs},Training Loss Epoch: {epoch_loss}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a6d142ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [00:15,  3.52it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs_all = []\n",
    "targets_all = []\n",
    "with torch.no_grad():\n",
    "    for _,data_res in tqdm(enumerate(all_data_loader, 0)):\n",
    "        ids = data_res['ids'].to(device, dtype = torch.long)\n",
    "        mask = data_res['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data_res['token_type_ids'].to(device, dtype=torch.long)\n",
    "        targets = data_res['targets'].to(device, dtype = torch.long)\n",
    "        embed = model_object.get_embed(ids, mask, token_type_ids)\n",
    "        outputs_all += embed.cpu().numpy().tolist()\n",
    "        targets_all += targets.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b5fd5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed_object = pd.DataFrame(outputs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a7b1276a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embed_object.to_csv('df_embed_object.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d56afc2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49e8d6ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "279de6c2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "052a0b3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff6f56eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5b3109c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7b52210",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ed4d574",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf2fe3f9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3ac2c86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b25a94da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec1f40a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ccbed27",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d83e1e28",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5356946",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93fd9fc0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d55348d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8d38cfc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63f49379",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0d43630",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5baa704",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e8c2e7d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d05e6bab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f3d2e0f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05fe8653",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

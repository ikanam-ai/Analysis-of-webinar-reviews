{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"itog_data.csv\")\n",
    "\n",
    "\n",
    "data['combined_questions'] = data['question_1'] + ' ' + data['question_2'] + ' ' + data['question_3'] + ' ' + data['question_4'] + ' ' + data['question_5']\n",
    "\n",
    "\n",
    "X = data['combined_questions']\n",
    "y_relevant = data['is_relevant']\n",
    "y_object = data['object']\n",
    "y_positive = data['is_positive']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train_relevant, y_test_relevant = train_test_split(X, y_relevant, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_object, y_test_object = train_test_split(X, y_object, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_positive, y_test_positive = train_test_split(X, y_positive, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 'is_relevant': 0.9923954372623575\n",
      "Accuracy for 'object': 0.9581749049429658\n",
      "Accuracy for 'is_positive': 0.9923954372623575\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_relevant = LogisticRegression()\n",
    "lr_relevant.fit(X_train_tfidf, y_train_relevant)\n",
    "y_pred_relevant = lr_relevant.predict(X_test_tfidf)\n",
    "accuracy_relevant = accuracy_score(y_test_relevant, y_pred_relevant)\n",
    "print(\"Accuracy for 'is_relevant':\", accuracy_relevant)\n",
    "\n",
    "\n",
    "lr_object = LogisticRegression()\n",
    "lr_object.fit(X_train_tfidf, y_train_object)\n",
    "y_pred_object = lr_object.predict(X_test_tfidf)\n",
    "accuracy_object = accuracy_score(y_test_object, y_pred_object)\n",
    "print(\"Accuracy for 'object':\", accuracy_object)\n",
    "\n",
    "lr_positive = LogisticRegression()\n",
    "lr_positive.fit(X_train_tfidf, y_train_positive)\n",
    "y_pred_positive = lr_positive.predict(X_test_tfidf)\n",
    "accuracy_positive = accuracy_score(y_test_positive, y_pred_positive)\n",
    "print(\"Accuracy for 'is_positive':\", accuracy_positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for 'is_relevant':\n",
      "Precision: 0.9940828402366864\n",
      "Recall: 0.9895833333333333\n",
      "F1-score: 0.9917606516290728\n",
      "\n",
      "Metrics for 'object':\n",
      "Precision: 0.9600032986970146\n",
      "Recall: 0.9581591779867642\n",
      "F1-score: 0.9585161739319954\n",
      "\n",
      "Metrics for 'is_positive':\n",
      "Precision: 0.9935897435897436\n",
      "Recall: 0.9908256880733946\n",
      "F1-score: 0.9921445639187574\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "precision_relevant, recall_relevant, f1_relevant = evaluate_model(y_test_relevant, y_pred_relevant)\n",
    "print(\"Metrics for 'is_relevant':\")\n",
    "print(\"Precision:\", precision_relevant)\n",
    "print(\"Recall:\", recall_relevant)\n",
    "print(\"F1-score:\", f1_relevant)\n",
    "\n",
    "\n",
    "precision_object, recall_object, f1_object = evaluate_model(y_test_object, y_pred_object)\n",
    "print(\"\\nMetrics for 'object':\")\n",
    "print(\"Precision:\", precision_object)\n",
    "print(\"Recall:\", recall_object)\n",
    "print(\"F1-score:\", f1_object)\n",
    "\n",
    "\n",
    "precision_positive, recall_positive, f1_positive = evaluate_model(y_test_positive, y_pred_positive)\n",
    "print(\"\\nMetrics for 'is_positive':\")\n",
    "print(\"Precision:\", precision_positive)\n",
    "print(\"Recall:\", recall_positive)\n",
    "print(\"F1-score:\", f1_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['combined_questions'] = df_test['question_1'] + ' ' + df_test['question_2'] + ' ' + df_test['question_3'] + ' ' + df_test['question_4'] + ' ' + df_test['question_5']\n",
    "\n",
    "test_x = tfidf_vectorizer.transform(df_test['combined_questions'])\n",
    "test_y_relevant = df_test['is_relevant']\n",
    "test_y_object = df_test['object']\n",
    "test_y_positive = df_test['is_positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8530219780219781, 0.8294871794871794, 0.8406629834254142)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_y_relevant, lr_relevant.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9244689221085759, 0.9229741019214703, 0.9221054705595068)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_y_object, lr_object.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8535947712418301, 0.8259036144578313, 0.8386591478696741)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(test_y_positive, lr_positive.predict(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
